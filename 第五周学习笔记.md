

# 第五周学习笔记

## linear-regression

```python
from torch import nn

from d2lzh_pytorch import data_iter, linreg, squared_loss, sgd

import torch
import numpy as np
__all__=['linear_regression_zero','linear_regression_simple']

true_w = [2, -3.4]
true_b = 4.2
# data
num_inputs = 2
num_examples = 1000

w = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float32, requires_grad=True)
b = torch.zeros(1, dtype=torch.float32, requires_grad=True)

features = torch.randn(num_examples, num_inputs,
                       dtype=torch.float32)
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()),
                       dtype=torch.float32)
lr = 0.03
num_epochs = 3
batch_size = 10


def linear_regression_zero():
    net = linreg
    loss = squared_loss

    for epoch in range(num_epochs):  # 训练模型一共需要num_epochs个迭代周期
        # 在每一个迭代周期中，会使用训练数据集中所有样本一次（假设样本数能够被批量大小整除）。X
        # 和y分别是小批量样本的特征和标签
        for X, y in data_iter(batch_size, features, labels):
            l = loss(net(X, w, b), y).sum()  # l是有关小批量X和y的损失
            l.backward()  # 小批量的损失对模型参数求梯度
            sgd([w, b], lr, batch_size)  # 使用小批量随机梯度下降迭代模型参数
            # 不要忘了梯度清零
            w.grad.data.zero_()
            b.grad.data.zero_()
        train_l = loss(net(features, w, b), labels)
        print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))


def linear_regression_simple():
    import torch.utils.data as Data
    dataset = Data.TensorDataset(features,labels)
    data_iter = Data.DataLoader(dataset,batch_size,shuffle=True)

    net = nn.Sequential(nn.Linear(num_inputs, 1))

    from torch.nn import init
    init.normal_(net[0].weight, mean=0, std=0.01)
    init.constant_(net[0].bias, val=0)  # 也可以直接修改bias的data: net[0].bias.data.fill_(0)

    loss = nn.MSELoss()
    import torch.optim as optim
    optimizer = optim.SGD(net.parameters(), lr=0.03)
    for epoch in range(num_epochs):
        for X, y in data_iter:
            output = net(X)
            l = loss(output, y.view(-1, 1))
            optimizer.zero_grad()  # 梯度清零，等价于net.zero_grad()
            l.backward()
            optimizer.step()
        print('epoch %d, loss: %f' % (epoch, l.item()))





if __name__ == '__main__':
    linear_regression_simple()
    
```

## softmax 实现分类任务

```python
import torch
import numpy as np

from d2lzh_pytorch import load_data_fashion_mnist, sgd
from d2lzh_pytorch.optimizer import evaluate_accuracy

batch_size = 256
train_iter, test_iter = load_data_fashion_mnist(batch_size)
num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)
b1 = torch.zeros(num_hiddens, dtype=torch.float)
W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)
b2 = torch.zeros(num_outputs, dtype=torch.float)
num_epochs, lr = 5, 100.0

params = [W1, b1, W2, b2]
for param in params:
    param.requires_grad_(requires_grad=True)


def net(X):
    X = X.view((-1, num_inputs))
    from d2lzh_pytorch.model import relu
    H = relu(torch.matmul(X, W1) + b1)
    return torch.matmul(H, W2) + b2


loss = torch.nn.CrossEntropyLoss()


def mlp_zero():
    for epoch in range(num_epochs):
        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0
        for X, y in train_iter:
            y_hat = net(X)
            l = loss(y_hat, y).sum()
            # 梯度清零
            params = [W1, b1, W2, b2]
            for param in params:
                param.grad.data.zero_()
            l.backward()
            sgd(params, lr, batch_size)
            train_l_sum += l.item()
            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()
            n += y.shape[0]
        test_acc = evaluate_accuracy(test_iter, net)
        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'
              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))


if __name__ == '__main__':
    mlp_zero()
    
```

## mlp 多层感知机

```python
import torch
import numpy as np

from d2lzh_pytorch import load_data_fashion_mnist, sgd
from d2lzh_pytorch.optimizer import evaluate_accuracy

batch_size = 256
train_iter, test_iter = load_data_fashion_mnist(batch_size)
num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)
b1 = torch.zeros(num_hiddens, dtype=torch.float)
W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)
b2 = torch.zeros(num_outputs, dtype=torch.float)
num_epochs, lr = 5, 100.0

params = [W1, b1, W2, b2]
for param in params:
    param.requires_grad_(requires_grad=True)


def net(X):
    X = X.view((-1, num_inputs))
    from d2lzh_pytorch.model import relu
    H = relu(torch.matmul(X, W1) + b1)
    return torch.matmul(H, W2) + b2


loss = torch.nn.CrossEntropyLoss()


def mlp_zero():
    for epoch in range(num_epochs):
        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0
        for X, y in train_iter:
            y_hat = net(X)
            l = loss(y_hat, y).sum()
            # 梯度清零
            params = [W1, b1, W2, b2]
            for param in params:
                param.grad.data.zero_()
            l.backward()
            sgd(params, lr, batch_size)
            train_l_sum += l.item()
            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()
            n += y.shape[0]
        test_acc = evaluate_accuracy(test_iter, net)
        print('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'
              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc))


if __name__ == '__main__':
    mlp_zero()
```

