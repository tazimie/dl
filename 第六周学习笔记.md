```python
import torch
from torch import nn

__all__ = ('AlexNet',)

from dl import load_data_fashion_mnist
from dl.train import train_fashion_mnist_gpu


class AlexNet(nn.Module):
    def __init__(self):
        super(AlexNet, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 96, 11, 4),  # in_channels, out_channels, kernel_size, stride, padding
            nn.ReLU(),
            nn.MaxPool2d(3, 2),  # kernel_size, stride
            # 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数
            nn.Conv2d(96, 256, 5, 1, 2),
            nn.ReLU(),
            nn.MaxPool2d(3, 2),
            # 连续3个卷积层，且使用更小的卷积窗口。除了最后的卷积层外，进一步增大了输出通道数。
            # 前两个卷积层后不使用池化层来减小输入的高和宽
            nn.Conv2d(256, 384, 3, 1, 1),
            nn.ReLU(),
            nn.Conv2d(384, 384, 3, 1, 1),
            nn.ReLU(),
            nn.Conv2d(384, 256, 3, 1, 1),
            nn.ReLU(),
            nn.MaxPool2d(3, 2)
        )
        # 这里全连接层的输出个数比LeNet中的大数倍。使用丢弃层来缓解过拟合
        self.fc = nn.Sequential(
            nn.Linear(256 * 5 * 5, 4096),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(),
            nn.Dropout(0.5),
            # 输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000
            nn.Linear(4096, 10),
        )

    def forward(self, img):
        feature = self.conv(img)
        output = self.fc(feature.view(img.shape[0], -1))
        return output


if __name__ == '__main__':
    batch_size = 128
    train_iter, test_iter = load_data_fashion_mnist(resize=224, batch_size=batch_size)
    lr, num_epochs = 0.001, 5
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    net = AlexNet()
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    train_fashion_mnist_gpu(net, train_iter, test_iter, loss, device, num_epochs, optimizer)
import torch
from torch import nn

from dl import load_data_fashion_mnist
from dl.train import train_fashion_mnist_gpu
from dl.model.Net import GlobalAvgPool2d, FlattenLayer

__all__ = ('DenseNet',)


def conv_block(in_channels, out_channels):
    blk = nn.Sequential(nn.BatchNorm2d(in_channels),
                        nn.ReLU(),
                        nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
    return blk


class DenseBlock(nn.Module):
    def __init__(self, num_convs, in_channels, out_channels):
        super(DenseBlock, self).__init__()
        net = []
        for i in range(num_convs):
            in_c = in_channels + i * out_channels
            net.append(conv_block(in_c, out_channels))
        self.net = nn.ModuleList(net)
        self.out_channels = in_channels + num_convs * out_channels  # 计算输出通道数

    def forward(self, X):
        for blk in self.net:
            Y = blk(X)
            X = torch.cat((X, Y), dim=1)  # 在通道维上将输入和输出连结
        return X


def transition_block(in_channels, out_channels):
    blk = nn.Sequential(
        nn.BatchNorm2d(in_channels),
        nn.ReLU(),
        nn.Conv2d(in_channels, out_channels, kernel_size=1),
        nn.AvgPool2d(kernel_size=2, stride=2))
    return blk


class DenseNet(nn.Module):
    def __init__(self):
        super(DenseNet, self).__init__()
        self.conv = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                                  nn.BatchNorm2d(64),
                                  nn.ReLU(),
                                  nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
        num_channels, growth_rate = 64, 32  # num_channels为当前的通道数
        num_convs_in_dense_blocks = [4, 4, 4, 4]
        self.dense = nn.Sequential()
        for i, num_convs in enumerate(num_convs_in_dense_blocks):
            DB = DenseBlock(num_convs, num_channels, growth_rate)
            self.dense.add_module("DenseBlosk_%d" % i, DB)
            # 上一个稠密块的输出通道数
            num_channels = DB.out_channels
            # 在稠密块之间加入通道数减半的过渡层
            if i != len(num_convs_in_dense_blocks) - 1:
                self.dense.add_module("transition_block_%d" % i, transition_block(num_channels, num_channels // 2))
                num_channels = num_channels // 2
        self.fc = nn.Sequential(nn.BatchNorm2d(num_channels), nn.ReLU(), GlobalAvgPool2d(), FlattenLayer(),
                                nn.Linear(num_channels, 10
                                          ))

    def forward(self, x):
        conv = self.conv(x)
        dense = self.dense(conv)
        return self.fc(dense)
        pass


if __name__ == '__main__':
    batch_size = 256
    # 如出现“out of memory”的报错信息，可减小batch_size或resize
    train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)
    net = DenseNet()
    lr, num_epochs = 0.001, 5
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    train_fashion_mnist_gpu(net, train_iter, test_iter, loss, device, num_epochs, optimizer)
import torch
from torch import nn
import torch.nn.functional as F

from dl import load_data_fashion_mnist
from dl.train import train_fashion_mnist_gpu
from dl.model.Net import GlobalAvgPool2d, FlattenLayer

__all__ = ("GoogleNet",)


class Inception(nn.Module):
    # c1 - c4为每条线路里的层的输出通道数
    def __init__(self, in_c, c1, c2, c3, c4):
        super(Inception, self).__init__()
        # 线路1，单1 x 1卷积层
        self.p1_1 = nn.Conv2d(in_c, c1, kernel_size=1)
        # 线路2，1 x 1卷积层后接3 x 3卷积层
        self.p2_1 = nn.Conv2d(in_c, c2[0], kernel_size=1)
        self.p2_2 = nn.Conv2d(c2[0], c2[1], kernel_size=3, padding=1)
        # 线路3，1 x 1卷积层后接5 x 5卷积层
        self.p3_1 = nn.Conv2d(in_c, c3[0], kernel_size=1)
        self.p3_2 = nn.Conv2d(c3[0], c3[1], kernel_size=5, padding=2)
        # 线路4，3 x 3最大池化层后接1 x 1卷积层
        self.p4_1 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)
        self.p4_2 = nn.Conv2d(in_c, c4, kernel_size=1)

    def forward(self, x):
        p1 = F.relu(self.p1_1(x))
        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))
        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))
        p4 = F.relu(self.p4_2(self.p4_1(x)))
        return torch.cat((p1, p2, p3, p4), dim=1)  # 在通道维上连结输出


class GoogleNet(nn.Module):

    def __init__(self):
        super(GoogleNet, self).__init__()
        b1 = nn.Sequential(nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
                           nn.ReLU(),
                           nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
        b2 = nn.Sequential(nn.Conv2d(64, 64, kernel_size=1),
                           nn.Conv2d(64, 192, kernel_size=3, padding=1),
                           nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
        b3 = nn.Sequential(Inception(192, 64, (96, 128), (16, 32), 32),
                           Inception(256, 128, (128, 192), (32, 96), 64),
                           nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
        b4 = nn.Sequential(Inception(480, 192, (96, 208), (16, 48), 64),
                           Inception(512, 160, (112, 224), (24, 64), 64),
                           Inception(512, 128, (128, 256), (24, 64), 64),
                           Inception(512, 112, (144, 288), (32, 64), 64),
                           Inception(528, 256, (160, 320), (32, 128), 128),
                           nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
        b5 = nn.Sequential(Inception(832, 256, (160, 320), (32, 128), 128),
                           Inception(832, 384, (192, 384), (48, 128), 128),
                           GlobalAvgPool2d())

        self.net = nn.Sequential(b1, b2, b3, b4, b5, FlattenLayer(), nn.Linear(1024, 10))

    def forward(self, x):
        return self.net(x)


if __name__ == '__main__':
    batch_size = 128
    # 如出现“out of memory”的报错信息，可减小batch_size或resize
    train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)
    net = GoogleNet()
    loss = nn.CrossEntropyLoss()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    lr, num_epochs = 0.001, 5
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    train_fashion_mnist_gpu(net, train_iter, test_iter, loss, device, num_epochs, optimizer)
import torch
from torch import nn

__all__ = ("LeNet",)

from dl import load_data_fashion_mnist
from dl.train import train_fashion_mnist, train_fashion_mnist_gpu


class LeNet(nn.Module):
    def __init__(self):
        super(LeNet, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 6, 5),  # in_channels, out_channels, kernel_size
            nn.Sigmoid(),
            nn.MaxPool2d(2, 2),  # kernel_size, stride
            nn.Conv2d(6, 16, 5),
            nn.Sigmoid(),
            nn.MaxPool2d(2, 2)
        )
        self.fc = nn.Sequential(
            nn.Linear(16 * 4 * 4, 120),
            nn.Sigmoid(),
            nn.Linear(120, 84),
            nn.Sigmoid(),
            nn.Linear(84, 10)
        )

    def forward(self, img):
        feature = self.conv(img)
        output = self.fc(feature.view(img.shape[0], -1))
        return output


if __name__ == '__main__':
    batch_size = 256
    train_iter, test_iter = load_data_fashion_mnist(batch_size=batch_size)
    lr, num_epochs = 0.001, 5
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    net = LeNet()
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    train_fashion_mnist_gpu(net, train_iter, test_iter, loss, device, num_epochs, optimizer)
import torch
from torch import nn

from dl import load_data_fashion_mnist
from dl.train import train_fashion_mnist_gpu
from dl.model.Net import FlattenLayer, GlobalAvgPool2d

__all__ = ("NiN",)


def nin_block(in_channels, out_channels, kernel_size, stride, padding):
    blk = nn.Sequential(nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),
                        nn.ReLU(),
                        nn.Conv2d(out_channels, out_channels, kernel_size=1),
                        nn.ReLU(),
                        nn.Conv2d(out_channels, out_channels, kernel_size=1),
                        nn.ReLU())
    return blk


class NiN(nn.Module):
    # NiN重复使用由卷积层和代替全连接层的1×1
    # 卷积层构成的NiN块来构建深层网络。
    # NiN去除了容易造成过拟合的全连接输出层，而是将其替换成输出通道数等于标签类别数的NiN块和全局平均池化层。
    # NiN的以上设计思想影响了后面一系列卷积神经网络的设计。
    def __init__(self):
        super(NiN, self).__init__()
        self.conv = nn.Sequential(
            nin_block(1, 96, kernel_size=11, stride=4, padding=0),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nin_block(96, 256, kernel_size=5, stride=1, padding=2),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nin_block(256, 384, kernel_size=3, stride=1, padding=1),
            nn.MaxPool2d(kernel_size=3, stride=2),
            nn.Dropout(0.5),
            # 标签类别数是10
            nin_block(384, 10, kernel_size=3, stride=1, padding=1),
        )
        self.fc = nn.Sequential(
            GlobalAvgPool2d(),
            # 将四维的输出转成二维的输出，其形状为(批量大小, 10)
            FlattenLayer())

    def forward(self, x):
        feature = self.conv(x)
        output = self.fc(feature)
        return output
        pass


if __name__ == '__main__':
    batch_size = 128
    # 如出现“out of memory”的报错信息，可减小batch_size或resize
    train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)
    net = NiN()
    lr, num_epochs = 0.002, 5
    loss = nn.CrossEntropyLoss()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    train_fashion_mnist_gpu(net, train_iter, test_iter, loss, device, num_epochs, optimizer)
import torch
from torch import nn
import torch.nn.functional as F

from dl import load_data_fashion_mnist, loss
from dl.train import train_fashion_mnist_gpu
from dl.model.Net import GlobalAvgPool2d, FlattenLayer

__all__ = ("ResNet",)


class Residual(nn.Module):  # 本类已保存在d2lzh_pytorch包中方便以后使用
    def __init__(self, in_channels, out_channels, use_1x1conv=False, stride=1):
        super(Residual, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1, stride=stride)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)
        if use_1x1conv:
            self.conv3 = nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride)
        else:
            self.conv3 = None
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.bn2 = nn.BatchNorm2d(out_channels)

    def forward(self, X):
        Y = F.relu(self.bn1(self.conv1(X)))
        Y = self.bn2(self.conv2(Y))
        if self.conv3:
            X = self.conv3(X)
        return F.relu(Y + X)


def resnet_block(in_channels, out_channels, num_residuals, first_block=False):
    if first_block:
        assert in_channels == out_channels  # 第一个模块的通道数同输入通道数一致
    blk = []
    for i in range(num_residuals):
        if i == 0 and not first_block:
            blk.append(Residual(in_channels, out_channels, use_1x1conv=True, stride=2))
        else:
            blk.append(Residual(out_channels, out_channels))
    return nn.Sequential(*blk)


class ResNet(nn.Module):
    # 残差块通过跨层的数据通道从而能够训练出有效的深度神经网络。
    # ResNet深刻影响了后来的深度神经网络的设计。
    def __init__(self):
        super(ResNet, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3),
            nn.BatchNorm2d(64),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=3, stride=2, padding=1))
        self.res = nn.Sequential(resnet_block(64, 64, 2, first_block=True),
                                 resnet_block(64, 128, 2),
                                 resnet_block(128, 256, 2),
                                 resnet_block(256, 512, 2))
        self.fc = nn.Sequential(GlobalAvgPool2d(), FlattenLayer(), nn.Linear(512, 10))

    def forward(self, x):
        conv = self.conv(x)
        res = self.res(conv)
        return self.fc(res)


if __name__ == '__main__':
    batch_size = 256
    # 如出现“out of memory”的报错信息，可减小batch_size或resize
    train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=96)
    net = ResNet()
    lr, num_epochs = 0.001, 5
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    train_fashion_mnist_gpu(net, train_iter, test_iter, loss, device, num_epochs, optimizer)
# VGG网络由卷积层模块后接全连接层模块构成。卷积层模块串联数个vgg_block
import torch
from torch import nn

from dl import load_data_fashion_mnist
from dl.train import train_fashion_mnist_gpu
from dl.model.Net import FlattenLayer

__all__ = ("VGGNet",)


def vgg_block(num_convs, in_channels, out_channels):
    blk = []
    for i in range(num_convs):
        if i == 0:
            blk.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
        else:
            blk.append(nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1))
        blk.append(nn.ReLU())
    blk.append(nn.MaxPool2d(kernel_size=2, stride=2))  # 这里会使宽高减半
    return nn.Sequential(*blk)


class VGGNet(nn.Module):
    # VGG - 11通过5个可以重复使用的卷积块来构造网络。根据每块里卷积层个数和输出通道数的不同可以定义出不同的VGG模型。
    # 现在我们构造一个VGG网络。它有5个卷积块，前2块使用单卷积层，而后3块使用双卷积层。
    # 第一块的输入输出通道分别是1（因为下面要使用的Fashion-MNIST数据的通道数为1）和64，
    # 之后每次对输出通道数翻倍，直到变为512。因为这个网络使用了8个卷积层和3个全连接层，所以经常被称为VGG-11。

    radio = 1

    def __init__(self):
        super(VGGNet, self).__init__()
        self.conv_arch = nn.Sequential(
            vgg_block(1, 1, 64 // self.radio),
            vgg_block(1, 64, 128 // self.radio),
            vgg_block(2, 128, 256 // self.radio),
            vgg_block(2, 256, 512 // self.radio),
            vgg_block(2, 512, 512 // self.radio)
        )
        self.fc_features = nn.Sequential(
            FlattenLayer(),
            # 经过5个vgg_block, 宽高会减半5次, 变成 224/32 = 7
            #
            nn.Linear(512 * 7 * 7 // self.radio, 4096),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(4096, 4096),
            nn.ReLU(),
            nn.Dropout(0.5),
            nn.Linear(4096, 10)
        )

    def forward(self, img):
        feature = self.conv_arch(img)
        output = self.fc_features(feature)
        return output


if __name__ == '__main__':
    batch_size = 64
    lr, num_epochs = 0.001, 5
    # 如出现“out of memory”的报错信息，可减小batch_size或resize
    train_iter, test_iter = load_data_fashion_mnist(batch_size, resize=224)
    net = VGGNet()
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    optimizer = torch.optim.Adam(net.parameters(), lr=lr)
    loss = nn.CrossEntropyLoss()
    train_fashion_mnist_gpu(net, train_iter, test_iter, loss, device, num_epochs, optimizer)
    pass
from . import *


```