# 深度学习实践 Hongpu liu

## 第一章 概述

### 机器学习概念

人工智能 使用算法进行prediction

#### machine learning主流的监督学习 有标注的数据进行训练 

- 常见算法
  - 穷举
  - 贪心
  - 分治
  - 动态规划

* 机器学习算法  计算过程来自于数据
  * DataSet + model + train

![image-20210302195250346](assets/image-20210302195250346.png)



#### 人工智能发展历史

1. rule-based system
2. classic machine learning
3. deep learning

![image-20210302200756006](assets/image-20210302200756006.png)

##### 维度诅咒 

feature 的数量越多需要越多的样本



![image-20210302201021769](assets/image-20210302201021769.png)

#### 传统机器学习策略

![image-20210302201051724](assets/image-20210302201051724.png)

#### 传统学习方法的问题

![image-20210302201316117](assets/image-20210302201316117.png)

#### 神经网络的历史  数学+工程系

![image-20210302202624734](assets/image-20210302202624734.png)

## 第二章 线性模型

![image-20210302203827666](assets/image-20210302203827666.png)

![image-20210302205331061](assets/image-20210302205331061.png)



## 第三章梯度下降

![image-20210302214004514](assets/image-20210302214004514.png)

- 随机梯度下降   根据每一个样本进行梯度下降   梯度下降能利用并行计算的优点 而随机梯度性能好  minibatch小批量随机梯度下降  使用小批量进行梯度下降  

## 第四章 反向传播 

## 第五章 用pytorch实现线性回归

![image-20210302222611639](assets/image-20210302222611639.png)



```python

```



![image-20210302225115636](assets/image-20210302225115636.png)



## 第六章逻辑回归

logistic函数 输出0-1的概率

![image-20200101082514391](assets/image-20200101082514391.png)





KL 散度   

cross-entropy 交叉熵  两个分布之间差距大小



## 第七章  处理多维输入的特征





## 第八章 加载数据集

random minibatch  random 鞍点  mini训练时间   batch 计算速度快

`epoch`:one forward pass and one backward pass of all training example

`batch-size`: the number of training example in one forward backw pass

`iteration`: number of passes，each pass using [batch size] number of example

`shuffle`打乱

![image-20210303170310768](assets/image-20210303170310768.png)

`Dataset` is an abstract class. We can define our class inherited from this class

`Dataloader`is a class to help us loading data in pytorch



![image-20210303170934461](assets/image-20210303170934461.png)



![image-20210303171843104](assets/image-20210303171843104.png)

![image-20210303172442799](assets/image-20210303172442799.png)

## 多分类问题

### `softamax` layer

![image-20210303180330260](assets/image-20210303180330260.png)



![image-20210303180954735](assets/image-20210303180954735.png)



- 均值和标准差    转换为 0-1分布的标准差 

- 数据归一化 标准化

![image-20210303181729760](assets/image-20210303181729760.png)



![image-20210303183116605](assets/image-20210303183116605.png)



## 第十二章 CNN

![image-20210303201304079](assets/image-20210303201304079.png)

![image-20210303204352114](assets/image-20210303204352114.png)



![image-20210303205639619](assets/image-20210303205639619.png)

![image-20210303215044488](assets/image-20210303215044488.png)



![image-20210303215147654](assets/image-20210303215147654.png)

![image-20210303215538339](assets/image-20210303215538339.png)

