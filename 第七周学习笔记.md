常见的一阶优化算法，本文约定符号如下： ![[公式]](https://www.zhihu.com/equation?tex=g) 为在mini-batch数据上求得的梯度， ![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta) 为模型参数， ![[公式]](https://www.zhihu.com/equation?tex=%5Cepsilon) 为学习率。

## SGD

![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+%5Cgets+%5Ctheta+-+%5Cepsilon+g)

最基本的方法，只用到当前时刻的梯度信息，每次都沿着当前的梯度下降方向往前走一小步。SGD有几个主要的问题：

- 学习率和学习率衰减方案对结果影响非常大，调起来麻烦
- 由于梯度方向往往不是指向最优解的（想象下椭圆形的梯度等高线），一般而言，我们希望梯度大的方向步子能小一点，梯度小的方向步子大一点，这样收敛会快，但是SGD每个方向学习率是一样的，所以下降方向容易震荡，导致收敛慢
- 容易陷入局部最优和鞍点

## SGD with momentum

![[公式]](https://www.zhihu.com/equation?tex=v+%5Cgets+%5Calpha+v+-+%5Cepsilon+g)

![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+%5Cgets+%5Ctheta+%2B+v)

带动量的SGD，不仅会利用当前时刻的梯度，同时会用到历史梯度信息，相当于把历史梯度信息按指数衰减进行加权平均，最后的效果就是加了“惯性”，梯度较大但是来回震荡的方向会互相抵消，梯度较小但是一直保持一致方向的会不断累积，这样可以避免下降方向来回震荡从而加速收敛（如下图所示），也有利于逃出鞍点。

![img](https://pic4.zhimg.com/80/v2-2476080e4cdfd489ae64ae3ceeafe48b_1440w.jpg)SGD without momentum

![img](https://pic4.zhimg.com/80/v2-b9388fd6e465d82687680f9d16edcd2b_1440w.jpg)SGD with momentum

## SGD with Nesterov momentum

![[公式]](https://www.zhihu.com/equation?tex=%5Chat+g+%5Cgets+f%27%28%5Ctheta%2B%5Calpha+v%29)

![[公式]](https://www.zhihu.com/equation?tex=v+%5Cgets+%5Calpha+v+-+%5Cepsilon+%5Chat+g)

![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+%5Cgets+%5Ctheta+%2B+v)

改进版的带动量的SGD，不同之处在于梯度的计算，梯度不是基于当前位置计算的，而是先按动量试探下一步的大概位置，然后估计梯度，其他步骤一样，这样做的好处是可以提前纠正动量方向的偏差。

## AdaGrad

![[公式]](https://www.zhihu.com/equation?tex=r+%5Cgets+r+%2B+g+%5Codot+g)

![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+%5Cgets+%5Ctheta+-+%5Cfrac%7B%5Cepsilon%7D%7B%5Cdelta+%2B+%5Csqrt%7Br%7D%7D+%5Codot+g)

自适应梯度，记录每一步的梯度平方和，用来调节不同参数的学习率，达到自适应学习率调节的目的。对于频繁更新的参数，学习率会越来越小，反之亦然。对于稀疏特征比较友好，比如训练词向量的时候可以试试。另一方面，由于可以自适应学习率，其实也可以减少震荡，因为震荡幅度大的话，累加的调节系数会越来越大，从而使震荡方向学习率降下来。虽然AdaGrad在凸优化上有一些比较好的性质，但是训练深度神经网络时，学习率容易在一开始就下降得过快导致效果不佳。

## RMSProp

![[公式]](https://www.zhihu.com/equation?tex=r+%5Cgets++%5Crho+r+%2B+%28+1+-+%5Crho%29+g%5Codot+g)

![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+%5Cgets+%5Ctheta+-+%5Cfrac%7B%5Cepsilon%7D%7B%5Cdelta+%2B+%5Csqrt%7Br%7D%7D+%5Codot+g)

全称Root Mean Square Prop，Hinton老爷子提出来的，这是一种没有正式文献发表但是在各大开源库中都有实现的方法。。RMSProp在AdaGrad的基础上加了指数衰减来缓解学习率下降过快的问题，在大部分问题上效果都不错。RMSProp当然也可以加动量项。

## Adam

![[公式]](https://www.zhihu.com/equation?tex=s+%5Cgets+%5Crho_1s+%2B+%281+-+%5Crho_1%29g)

![[公式]](https://www.zhihu.com/equation?tex=r+%5Cgets+%5Crho_2r+%2B+%281+-+%5Crho_2%29+g+%5Codot+g)

![[公式]](https://www.zhihu.com/equation?tex=%5Chat+s+%5Cgets+%5Cfrac%7Bs%7D%7B1+-+%5Crho_1%5Et%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5Chat+r+%5Cgets+%5Cfrac%7Br%7D%7B1+-+%5Crho_2%5Et%7D)

![[公式]](https://www.zhihu.com/equation?tex=%5Ctheta+%5Cgets+%5Ctheta+-+%5Cepsilon%5Cfrac%7B%5Chat+s%7D%7B%5Cdelta+%2B+%5Csqrt%7B%5Chat+r%7D%7D)

全称adaptive momentum，另一种自适应学习率的方法，和RMSProp的不同主要有两点，一方面，不直接使用当前梯度，而是使用历史加权平均梯度；另一方面，对加权后的一阶动量和二阶动量都做校正，相当于得到一个无偏估计，不像RMSProp在早期的时候bias会比较大。Adam的超参数也比较好调，大部分时候用默认参数就行。但是，ICLR2018爆出Adam的收敛性证明有问题，而且给出了实锤counter example，在一些简单的问题上没法收敛到最优解，作者提出了一种fix的方法AMSGrad，但是实测效果好像和Adam差不多，但这不影响文章拿了best paper。ICLR2019最近放出的文章里，又有一篇是讲Adam的，貌似又提出了一种新的方法叫AdaShift，不知道效果咋样。。坐等各路大神测试。。