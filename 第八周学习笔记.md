# pytorch 文档

pytorch 是一个优化的张量库，用于使用学习GPU和CPU

## 说明

### 自动求导机制

#### 排除子图 

require volatile它们都允许从梯度计算中精细地排除子图，并可以提高效率。

如果有一个单一的输入操作需要梯度，它的输出也需要梯度。

```python
model = torchvision.models.resnet18(pretrained=True)
for param in model.parameters():
    param.requires_grad = False
# Replace the last fully-connected layer
# Parameters of newly constructed modules have requires_grad=True by default
model.fc = nn.Linear(512, 100)

# Optimize only the classifier
optimizer = optim.SGD(model.fc.parameters(), lr=1e-2, momentum=0.9)
```

`volatile`不同于`require_grad`的传递。如果一个操作甚至只有有一个`volatile`的输入，它的输出也将是`volatile`。

#### 自动求导如何编码历史信息

每个变量都有一个`.creator`属性，它指向把它作为输出的函数。

#### variable上的In-place操作

自动求导中支持in-place操作是一件很困难的事情，我们在大多数情况下都不鼓励使用它们。

#### in-place正确性检查

### CUDA语义

`torch.cuda`会记录当前选择的GPU，并且分配的所有CUDA张量将在上面创建。可以使用`torch.cuda.device`上下文管理器更改所选设备。

默认情况下，不支持跨GPU操作，唯一的例外是`copy_()`。 除非启用对等存储器访问，否则对分布不同设备上的张量任何启动操作的尝试都将会引发错误。

```python
x = torch.cuda.FloatTensor(1)
# x.get_device() == 0
y = torch.FloatTensor(1).cuda()
# y.get_device() == 0

with torch.cuda.device(1):
    # allocates a tensor on GPU 1
    a = torch.cuda.FloatTensor(1)

    # transfers a tensor from CPU to GPU 1
    b = torch.FloatTensor(1).cuda()
    # a.get_device() == b.get_device() == 1

    c = a + b
    # c.get_device() == 1

    z = x + y
    # z.get_device() == 0

    # even within a context, you can give a GPU id to the .cuda call
    d = torch.randn(2).cuda(2)
    # d.get_device() == 2
```



### 扩展Pytorch

扩展torch.autograd

```python
# Inherit from Function
class Linear(Function):

    # bias is an optional argument
    def forward(self, input, weight, bias=None):
        self.save_for_backward(input, weight, bias)
        output = input.mm(weight.t())
        if bias is not None:
            output += bias.unsqueeze(0).expand_as(output)
        return output

    # This function has only a single output, so it gets only one gradient
    def backward(self, grad_output):
        # This is a pattern that is very convenient - at the top of backward
        # unpack saved_tensors and initialize all gradients w.r.t. inputs to
        # None. Thanks to the fact that additional trailing Nones are
        # ignored, the return statement is simple even when the function has
        # optional inputs.
        input, weight, bias = self.saved_tensors
        grad_input = grad_weight = grad_bias = None

        # These needs_input_grad checks are optional and there only to
        # improve efficiency. If you want to make your code simpler, you can
        # skip them. Returning gradients for inputs that don't require it is
        # not an error.
        if self.needs_input_grad[0]:
            grad_input = grad_output.mm(weight)
        if self.needs_input_grad[1]:
            grad_weight = grad_output.t().mm(input)
        if bias is not None and self.needs_input_grad[2]:
            grad_bias = grad_output.sum(0).squeeze(0)

        return grad_input, grad_weight, grad_bias
      
      
from torch.autograd import gradcheck

# gradchek takes a tuple of tensor as input, check if your gradient
# evaluated with these tensors are close enough to numerical
# approximations and returns True if they all verify this condition.
input = (Variable(torch.randn(20,20).double(), requires_grad=True),)
test = gradcheck.gradcheck(Linear(), input, eps=1e-6, atol=1e-4)
print(test)      
```

```python
class Linear(nn.Module):
    def __init__(self, input_features, output_features, bias=True):
        self.input_features = input_features
        self.output_features = output_features

        # nn.Parameter is a special kind of Variable, that will get
        # automatically registered as Module's parameter once it's assigned
        # as an attribute. Parameters and buffers need to be registered, or
        # they won't appear in .parameters() (doesn't apply to buffers), and
        # won't be converted when e.g. .cuda() is called. You can use
        # .register_buffer() to register buffers.
        # nn.Parameters can never be volatile and, different than Variables,
        # they require gradients by default.
        self.weight = nn.Parameter(torch.Tensor(input_features, output_features))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(output_features))
        else:
            # You should always register all possible parameters, but the
            # optional ones can be None if you want.
            self.register_parameter('bias', None)

        # Not a very smart way to initialize weights
        self.weight.data.uniform_(-0.1, 0.1)
        if bias is not None:
            self.bias.data.uniform_(-0.1, 0.1)

    def forward(self, input):
        # See the autograd section for explanation of what happens here.
        return Linear()(input, self.weight, self.bias)
        #注意这个Linear是之前实现过的Linear
```



### 多进程最佳实践

`torch.multiprocessing`是Python`multiprocessing`的替代品。它支持完全相同的操作，但扩展了它以便通过`multiprocessing.Queue`发送的所有张量将其数据移动到共享内存中，并且只会向其他进程发送一个句柄。

### 序列化语义

## Packagae reference

### torch

- Tensors

- creation Ops

- Indexing，Slicing，Joining，Mutating Ops

- Random sampling

- Serialization 

- Parallelism

- Math operations

- Pointwise Ops

- Reduction Ops

- Comparision Ops

- Other Ops

- BlAS and LAPCK Operations



### torch.Tensor

​	`torch.Tensor`是一种包含单一数据类型元素的多维矩阵。

​	Torch定义了七种CPU tensor类型和八种GPU tensor类型：

### torch.Storage 存储

一个`torch.Storage`是一个单一数据类型的连续一维数组。

每个`torch.Tensor`都有一个对应的、相同数据类型的存储。

### torch.nn 神经网络层

- parameters `Variable`的一种，常被用于模块参数(`module parameter`)。
- containers `nn.module` 所有网络的基类
- 卷积层
- 池化层
- Non-Linear Activations
- RNN layers
  - RNN
  - LSTM
  - GRU
- Linear layer
- Sparse layer
- distance functions
- loss functions
- Vision layers
- Multi-GPU layers
- Utils

### torch.nn.functional 函数库

- Convolution 函数
- Pooling
- 非线性激活函数
- Normalization
- 线性函数
- Dropout函数
- 距离函数（Distance functions）
- 损失函数（Loss functions）
- Vision functions

### torch.nn.init 参数初始化

对于给定的非线性函数，返回推荐的增益值

### torch.optim 优化算法

### torch.autograd

`torch.autograd`提供了类和函数用来对任意标量函数进行求导。要想使用自动求导，只需要对已有的代码进行微小的改变。只需要将所有的`tensor`包含进`Variable`对象中即可。

### torch.multiprocessing

这个API与原始模型完全兼容，为了让张量通过队列或者其他机制共享，移动到内存中，我们可以

由原来的`import multiprocessing`改为`import torch.multiprocessing`。

### torch.legacy 此包中包含从Lua Torch移植来的代码。

### torch.cuda 

该包增加了对CUDA张量类型的支持，实现了与CPU张量相同的功能，但使用GPU进行计算。

它是懒惰的初始化，所以你可以随时导入它，并使用`is_available()`来确定系统是否支持CUDA。

[CUDA语义](https://pytorch-cn.readthedocs.io/zh/latest/notes/cuda/)中有关于使用CUDA的更多细节。

### torch.utils.fft

创建并配置一个cffi.FFI对象,用于PyTorch的扩展。

### torch.utils.data 数据加载

### torch.utils.model_zoo

在给定URL上加载Torch序列化对象

## torch reference

### torchvision

`torchvision`包 包含了目前流行的数据集，模型结构和常用的图片转换工具。

### torchvision.datasets

- MNIST
- COCO（用于图像标注和目标检测）(Captioning and Detection)
- LSUN Classification
- ImageFolder
- Imagenet-12
- CIFAR10 and CIFAR100
- STL10

### torchvision.models

`torchvision.models`模块的 子模块中包含以下模型结构。

- AlexNet
- VGG
- ResNet
- SqueezeNet
- DenseNet You can construct a model with random weights by calling its constructor:

### torchvision.transforms

pytorch torchvision transform

对PIL.Image进行变换

### torchvision.utils





### 